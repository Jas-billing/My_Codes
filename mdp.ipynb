{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e868b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JASKEE~1\\AppData\\Local\\Temp/ipykernel_28104/2313961103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mv1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum_sr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JASKEE~1\\AppData\\Local\\Temp/ipykernel_28104/2313961103.py\u001b[0m in \u001b[0;36msum_sr\u001b[1;34m(v, s, a, gamma)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0ms_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;31m# p  - transition probability from (s,a) to (s')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m# s_ - next state (s')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\JASKEE~1\\AppData\\Local\\Temp/ipykernel_28104/2313961103.py\u001b[0m in \u001b[0;36msample_reward\u001b[1;34m(self, current_state_action_tuple)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_state_action_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0maction_tuple_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_state_action\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mcurrent_state_action_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_tuple_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_tuple_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "'''\n",
    "The MDP has 6 states: 0,1,2,3,4,5. The state 5 is the terminal state and the system can start in any state among 0 to 4.\n",
    "Possible actions and their resultant landing state and rewards are given in the arrays defined in the class.\n",
    "For each current_state_action tuple allowed, the corresponding next state and reward is given by the element having the same\n",
    "index as the tuple in its array.\n",
    "To access the environment, we create an object of it.\n",
    "To simulate, you pass a state-action tuple (s,a) to the sample_reward() function, which updates the current state and returns a reward.\n",
    "For simplicity of handling, actions have been set as integers as well. Actions are 0,1,2 (sorry for possible confusion \n",
    "with state indices but this ensures easier implementation). Allowed actions from each state are given in the current_state_action tuple array.\n",
    "In each tuple in that array, the first element corresponds to current state and the second \n",
    "element corresponds to the action.\n",
    "'''\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self.start_state=np.random.randint(low=0,high=4,dtype=int,size=1)[0]\n",
    "        self.terminal_state=5\n",
    "        self.state=self.start_state\n",
    "        self.done=False\n",
    "        self.current_state_action=np.array([(0,2),(0,1),(1,0),(1,1),(2,0),(2,1),(3,0),(3,1),(4,0)])\n",
    "        self.next_state=np.array([5,1,0,2,1,3,2,4,3])\n",
    "        self.rewards=np.array([-2,-1,+1,-1,+1,-1,+1,-1,+1])\n",
    "\n",
    "    def close(self):\n",
    "        self.state=self.terminal_state\n",
    "\n",
    "    def sample_reward(self, current_state_action_tuple):\n",
    "        action_tuple_index=np.where(self.current_state_action==current_state_action_tuple)\n",
    "        self.state=self.next_state(action_tuple_index)\n",
    "        reward=self.rewards[action_tuple_index]\n",
    "        return reward\n",
    "\n",
    "'''\n",
    "Your objective is to learn the optimal policy and the associated optimal value function.\n",
    "Note that the state number 5 has been set as the terminal state so it\n",
    "should have the value 0. \n",
    "For states 1,2,3, there are two possible actions (0,1). For state 4, there is only possible action (0).\n",
    "For state 0, there are two actions (1,2), but the behaviour is different from states 1,2,3.\n",
    "'''\n",
    "\n",
    "env_obj=Environment()\n",
    "\n",
    "'''\n",
    "What are valid operation that you can do?\n",
    "1. Accessing the current state using state attribute of the environment object.\n",
    "2. View the current_state_action array and chose which action to take.\n",
    "3. To reach the next state, call the function sample_reward of the Environment class. It sets \n",
    "the next state in the state attribute of the object and returns a numerical reward.\n",
    "'''\n",
    "\n",
    "#-------Your code starts here---------------\n",
    "def sum_sr(v, s, a, gamma):\n",
    "    \"\"\"Calc state-action value for state 's' and action 'a'\"\"\"\n",
    "    tmp = env_obj.start_state # state value for state s\n",
    "    p=1\n",
    "    s_=env_obj.next_state\n",
    "    r=env_obj.sample_reward((s,a))\n",
    "# p  - transition probability from (s,a) to (s')\n",
    "# s_ - next state (s')\n",
    "# r  - reward on transition from (s,a) to (s')\n",
    "    tmp += p * (r + gamma * v[s_])\n",
    "    return tmp\n",
    "n_states=6\n",
    "n_actions=3\n",
    "#taking transition state probabilities to be 1......\n",
    "gamma=0.75 # gamma\n",
    "policy =[0 for s in range(n_states)]\n",
    "v=np.zeros(n_states)\n",
    "pi=np.zeros(n_states,dtype=int)\n",
    "is_value_changed=True\n",
    "iterations=0\n",
    "theta=1e-8\n",
    "while is_value_changed:\n",
    "    while True:\n",
    "        delta=0\n",
    "        for s in range(n_states):\n",
    "            v1=v[s]\n",
    "            v[s]=sum_sr(v=v,s=s,a=pi[s],gamma=gamma)\n",
    "            delta=max(delta,abs(v1-v[s]))\n",
    "        if delta < theta: break\n",
    "        \n",
    "    policy_stable=True\n",
    "    for s in range(n_states):\n",
    "        for a in range(n_actions):\n",
    "            old_action=pi[s]\n",
    "            pi[s]=np.argmax([sum_sr(v=v,s=s,a=a,gamma=gamma)]) \n",
    "            if old_action != pi[s]: policy_stable = False\n",
    "        if policy_stable: break\n",
    "print(pi)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8aca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c4417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
