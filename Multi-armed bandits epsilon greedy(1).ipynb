{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b7ca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean rewards [0.26 0.06 0.43 1.22 2.96 0.17 0.19 1.59 1.6 ]\n",
      "Estimated mean rewards [0.88912698 0.88891667 0.70982906 1.38761905 2.91901907 0.90530612\n",
      " 0.76888    1.44557522 1.78255319]\n",
      "Error in mean reward prediction for each arm: [0.62912698 0.82891667 0.27982906 0.16761905 0.04098093 0.73530612\n",
      " 0.57888    0.14442478 0.18255319]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "std=1\n",
    "mean=1\n",
    "'''\n",
    "This class simply generates a bandit problem with number of bandit arms\n",
    "being `num_bandits`, which itself is chosen randomly. The numbers being sampled\n",
    "from the gaussian are the rewards associated with those arms.\n",
    "For simplicity, I have restricted ourselved to nonnegative rewards and decimals upto 2 digits\n",
    "to avoid any floating point calculation errors.\n",
    "'''\n",
    "class Bandit_arms:\n",
    "\n",
    "    def __init__(self, num_bandits):\n",
    "        self.bandits=np.round(np.absolute(np.random.normal(loc=mean, scale=std, size=num_bandits)), decimals=2)\n",
    "\n",
    "    def return_array(self):\n",
    "        return self.bandits\n",
    "\n",
    "\n",
    "'''\n",
    "Minimum: 5 bandit arms and maximum: 20 bandit arms.\n",
    "'''\n",
    "\n",
    "num_bandits=np.random.randint(low=5, high=20,size=1)[0]\n",
    "bandit_problem=Bandit_arms(num_bandits).return_array()\n",
    "print(\"True mean rewards\",bandit_problem)\n",
    "\n",
    "'''\n",
    "This is technically cheating, you should not be aware of the true underlying \n",
    "reward through any means. It doesn't matter as beginners but keep it in mind\n",
    "that this data is unavailable to you (no need for learning if we knew already!).\n",
    "'''\n",
    "\n",
    "'''\n",
    "Now for each arm, when we pull it, we get modulus of number sampled\n",
    "from a gaussian with mean defined in `bandit_problem` array and standard\n",
    "deviation=std.\n",
    "Keep in mind that first bandit arm would have index 0.\n",
    "'''\n",
    "\n",
    "def bandit_simulator(arm_index):\n",
    "    reward=np.round(np.absolute(np.random.normal(loc=bandit_problem[arm_index], scale=std, size=1)), decimals=2)[0]\n",
    "    return reward\n",
    "\n",
    "\n",
    "'''\n",
    "So, now we have an environment that generates a bandit problem\n",
    "with a random number of arms and random mean rewards. We have a function to \n",
    "sample reward on \"pulling\" each of these arms. Now, try out algorithms that \n",
    "can come close to true mean rewards.\n",
    "Any rule based method won't work, since means change everytime you run the python file.\n",
    "So, your algorithm must truly be able to learn as good as it can in a single run of this file.\n",
    "As you must have understood by now, learning is an iterative procedure. Typically, to represent limited\n",
    "computational and time resources, upper limits on allowed learning iterations are imposed. So, I am setting \n",
    "a variable that defines number of allowed iterations. Of course, while building, you can play around with it.\n",
    "'''\n",
    "\n",
    "num_allowed_iterations=2000\n",
    "\n",
    "'''\n",
    "Except for the values of mean, std, num_iterations, I don't think you should\n",
    "have the need to change any of the code written to this point.\n",
    "'''\n",
    "\n",
    "estimated_means=np.zeros(shape=num_bandits, dtype=float)\n",
    "#--------------------------\n",
    "# the part that truly matters\n",
    "N=np.zeros(shape=num_bandits, dtype=int) #number of times each arm was pulled\n",
    "epsilon = 0.5\n",
    "arr=np.arange(0,num_bandits) #array for the arms\n",
    "for pull in range (1,num_allowed_iterations):\n",
    "    r_pull=[] #all rewards for this iter\n",
    "    if random.random()<epsilon:\n",
    "        a=np.random.choice(arr,size=None) #exploration\n",
    "    else:\n",
    "        a=np.argmax(estimated_means) #exploitation\n",
    "    temp_r=bandit_simulator(a)\n",
    "    N[a]=N[a]+1\n",
    "    estimated_means[a]=estimated_means[a]+(temp_r-estimated_means[a])/N[a]\n",
    "print(\"Estimated mean rewards\",estimated_means)\n",
    "'''\n",
    "YOUR ALGORITHM MUST NOT USE THE TRUE BANDIT MEAN REWARD ARRAY AT ANY STEP.\n",
    "`num_bandits` VARIABLE PROVIDES YOU THE NUMBER OF BANDITS ARMS AND SIMPLY CALL THE\n",
    "SIMULATOR FUNCTION. ANY ALGORITHM ACCESSING MEAN REWARD ARRAY IS OBVIOUSLY WRONG.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Underlying function checks how well your estimate is. Since, the learning is completed by this point\n",
    "so accessing the true mean for checking is somewhat acceptable.\n",
    "'''\n",
    "def check(bandit_problem, estimate_means):\n",
    "    errors=np.absolute(estimate_means-bandit_problem)\n",
    "    print(\"Error in mean reward prediction for each arm:\",errors)\n",
    "\n",
    "check(bandit_problem, estimated_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e411b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
